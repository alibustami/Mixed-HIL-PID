# Mixed Human-in-the-Loop Optimization: Formal Pseudocode

## Abstract

This document presents the formal algorithmic specification of the **Mixed Human-in-the-Loop (Mixed HIL)** optimization framework for automated PID controller tuning. The approach synergistically combines Differential Evolution (DE) and Bayesian Optimization (BO) with adaptive preference learning to incorporate human expertise into the optimization process.

---

## Algorithm 1: Main Mixed HIL Framework

```
Algorithm 1: Mixed HIL Optimization Framework
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  bounds âˆˆ â„â¿Ë£Â² - Parameter bounds [min, max] for n parameters
        max_iter - Maximum optimization iterations
        Î±_pref - Preference learning rate
        Fâ‚€ - Initial DE mutation factor
        N_pop - DE population size
        Ï_min - Minimum BO probability of feasibility
        
Output: Î¸* - Optimal PID parameters
        H - Complete optimization history
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === INITIALIZATION ===
2:  w â† RANDOM([0,1]â¿)                    â–· Initialize preference weights
3:  Î¸_anchor â† bounds_min + w âŠ™ (bounds_max - bounds_min)
4:  
5:  DE â† InitializeDifferentialEvolution(bounds, N_pop, Fâ‚€)
6:  DE.population[0] â† Î¸_anchor            â–· Seed with preference anchor
7:  
8:  BO â† InitializeBayesianOptimizer(bounds, Ï_min)
9:  
10: // Warm-start BO with DE's initial population
11: for each Î¸ âˆˆ DE.population do
12:     J, g â† EvaluatePID(Î¸)              â–· J: cost, g: constraint violation
13:     BO.Update(Î¸, J, g)
14: end for
15: 
16: Î¸_best â† argmin{J(Î¸) | g(Î¸) â‰¤ 0}       â–· Best feasible solution
17: H â† âˆ…                                   â–· Optimization history
18: 
19: // === MAIN OPTIMIZATION LOOP ===
20: for i â† 1 to max_iter do
21:     
22:     // === CANDIDATE GENERATION ===
23:     Î¸_DE, J_DE, g_DE â† DE.Evolve()     â–· DE proposes candidate A
24:     Î¸_BO â† BO.Propose()                 â–· BO proposes candidate B
25:     
26:     J_BO, g_BO â† EvaluatePID(Î¸_BO)
27:     
28:     // === KNOWLEDGE SHARING ===
29:     BO.Update(Î¸_DE, J_DE, g_DE)         â–· BO learns from DE
30:     BO.Update(Î¸_BO, J_BO, g_BO)         â–· BO learns from itself
31:     
32:     // === DETAILED EVALUATION ===
33:     response_DE â† SimulatePID(Î¸_DE)     â–· Full system response
34:     response_BO â† SimulatePID(Î¸_BO)
35:     
36:     metrics_DE â† ComputeMetrics(response_DE)  â–· Overshoot, rise time, etc.
37:     metrics_BO â† ComputeMetrics(response_BO)
38:     
39:     // === TERMINATION CHECK ===
40:     if PerformanceTargetsMet(metrics_DE) âˆ§ g_DE â‰¤ 0 then
41:         return Î¸_DE, H âˆª {(i, Î¸_DE, "auto_terminate_de")}
42:     end if
43:     
44:     if PerformanceTargetsMet(metrics_BO) âˆ§ g_BO â‰¤ 0 then
45:         return Î¸_BO, H âˆª {(i, Î¸_BO, "auto_terminate_bo")}
46:     end if
47:     
48:     // === HUMAN FEEDBACK ===
49:     choice â† HumanComparison(response_DE, response_BO, 
50:                               Î¸_DE, Î¸_BO, metrics_DE, metrics_BO)
51:     
52:     // === ADAPTIVE RESPONSE TO FEEDBACK ===
53:     switch choice do
54:         case PREFER_DE:
55:             gap â† PreferenceUpdate(w, Î¸_DE, Î¸_BO, Î±_pref)
56:             Î¸_anchor â† ComputeAnchor(w, bounds)
57:             
58:             DE.mutation â† Fâ‚€                    â–· Reset exploration
59:             DE.InjectCandidate(Î¸_anchor)        â–· Inject learned anchor
60:             
61:             BO.PreferenceNudge(Î¸_DE, J_DE, J_BO, g_DE)
62:             H â† H âˆª {(i, "prefer_de", Î¸_DE, Î¸_BO, gap)}
63:             
64:         case PREFER_BO:
65:             gap â† PreferenceUpdate(w, Î¸_BO, Î¸_DE, Î±_pref)
66:             Î¸_anchor â† ComputeAnchor(w, bounds)
67:             
68:             DE.mutation â† Fâ‚€
69:             DE.InjectCandidate(Î¸_BO)            â–· Share BO's solution
70:             DE.InjectCandidate(Î¸_anchor)        â–· Also inject anchor
71:             
72:             BO.PreferenceNudge(Î¸_BO, J_BO, J_DE, g_BO)
73:             H â† H âˆª {(i, "prefer_bo", Î¸_BO, Î¸_DE, gap)}
74:             
75:         case TIE_REFINE:
76:             Î¸_mid â† (Î¸_DE + Î¸_BO) / 2           â–· Midpoint
77:             
78:             DE.RefineSearchSpace(Î¸_mid, 0.5)    â–· Shrink 50%
79:             BO.RefineBounds(Î¸_mid, 0.5)
80:             
81:             H â† H âˆª {(i, "tie_refine", Î¸_mid)}
82:             
83:         case REJECT_BOTH:
84:             DE.ExpandSearchSpace(1.5)           â–· Expand 50%
85:             BO.ExpandBounds(1.5)
86:             
87:             H â† H âˆª {(i, "reject_expand")}
88:             
89:         case EXIT:
90:             return Î¸_best, H
91:     end switch
92:     
93:     // === UPDATE BEST ===
94:     Î¸_best â† UpdateBest(Î¸_best, Î¸_DE, Î¸_BO)   â–· Feasibility-aware
95:     
96: end for
97: 
98: return Î¸_best, H
```

---

## Algorithm 2: Preference Learning Mechanism

```
Algorithm 2: Preference Weight Update
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  w âˆˆ [0,1]â¿ - Current preference weights
        Î¸_pref âˆˆ â„â¿ - Preferred candidate
        Î¸_other âˆˆ â„â¿ - Other candidate
        Î± âˆˆ (0,1) - Learning rate
        bounds âˆˆ â„â¿Ë£Â² - Parameter bounds
        
Output: gap âˆˆ â„â¿ - Normalized distance vector
        w_new âˆˆ [0,1]â¿ - Updated weights
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === NORMALIZATION ===
2:  span â† bounds_max - bounds_min
3:  Î¸Ì‚_pref â† (Î¸_pref - bounds_min) / (span + Îµ)   â–· Îµ = 10â»â¹
4:  Î¸Ì‚_other â† (Î¸_other - bounds_min) / (span + Îµ)
5:  
6:  // === GAP COMPUTATION ===
7:  gap â† Î¸Ì‚_other - Î¸Ì‚_pref                         â–· Direction vector
8:  
9:  // === EXPONENTIAL MOVING AVERAGE UPDATE ===
10: w_new â† w + Î± Â· (Î¸Ì‚_pref - w)                   â–· Shift toward preference
11: w_new â† CLIP(w_new, 0, 1)                       â–· Ensure [0,1] bounds
12: 
13: return gap, w_new


Algorithm 3: Anchor Point Generation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  w âˆˆ [0,1]â¿ - Preference weights
        bounds âˆˆ â„â¿Ë£Â² - Parameter bounds
        
Output: Î¸_anchor âˆˆ â„â¿ - Anchor point in parameter space
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  Î¸_anchor â† bounds_min + w âŠ™ (bounds_max - bounds_min)
2:  return Î¸_anchor
```

---

## Algorithm 3: Differential Evolution with HIL Adaptations

```
Algorithm 4: Feasibility-Aware Differential Evolution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  bounds âˆˆ â„â¿Ë£Â² - Adaptive search bounds
        global_bounds âˆˆ â„â¿Ë£Â² - Hard domain limits
        N_pop - Population size
        F - Mutation factor
        CR - Crossover probability
        
Output: Î¸_best - Best candidate from current generation
        J_best - Best objective value
        g_best - Best constraint violation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === INITIALIZATION (if needed) ===
2:  if population = âˆ… then
3:      for i â† 1 to N_pop do
4:          P[i] â† bounds_min + RANDOM([0,1]â¿) âŠ™ (bounds_max - bounds_min)
5:          P[i] â† CLIP(P[i], global_bounds_min, global_bounds_max)
6:          J[i] â† âˆ, g[i] â† âˆ
7:      end for
8:  end if
9:  
10: // === EVOLUTION STEP ===
11: for i â† 1 to N_pop do
12:     
13:     // === MUTATION (DE/rand/1) ===
14:     râ‚, râ‚‚, râ‚ƒ â† RANDOM_DISTINCT({1,...,N_pop} \ {i})
15:     v â† P[râ‚] + F Â· (P[râ‚‚] - P[râ‚ƒ])
16:     v â† CLIP(v, bounds_min, bounds_max)
17:     v â† CLIP(v, global_bounds_min, global_bounds_max)
18:     
19:     // === CROSSOVER (binomial) ===
20:     u â† P[i]
21:     j_rand â† RANDOM({1,...,n})
22:     for j â† 1 to n do
23:         if RANDOM() < CR âˆ¨ j = j_rand then
24:             u[j] â† v[j]
25:         end if
26:     end for
27:     
28:     // === EVALUATION ===
29:     J_trial, g_trial â† EvaluatePID(u)
30:     
31:     // === FEASIBILITY-AWARE SELECTION ===
32:     if IsBetter(J_trial, g_trial, J[i], g[i]) then
33:         P[i] â† u
34:         J[i] â† J_trial
35:         g[i] â† g_trial
36:     end if
37:     
38: end for
39: 
40: // === FIND BEST (feasibility-aware) ===
41: idx_best â† argmin_i {J[i] | using feasibility rules}
42: return P[idx_best], J[idx_best], g[idx_best]


Algorithm 5: Feasibility-Aware Comparison
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  J_a, g_a - Objective and constraint for candidate A
        J_b, g_b - Objective and constraint for candidate B
        
Output: true if A is better than B, false otherwise
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  feas_a â† (g_a â‰¤ 0)
2:  feas_b â† (g_b â‰¤ 0)
3:  
4:  // === FEASIBILITY RULES (Deb et al.) ===
5:  if feas_a âˆ§ Â¬feas_b then
6:      return true                          â–· Feasible beats infeasible
7:  end if
8:  
9:  if Â¬feas_a âˆ§ feas_b then
10:     return false
11: end if
12: 
13: if feas_a âˆ§ feas_b then
14:     return J_a < J_b                     â–· Both feasible: compare cost
15: end if
16: 
17: return g_a < g_b                         â–· Both infeasible: less violation
```

---

## Algorithm 4: Bayesian Optimization with Constraints

```
Algorithm 6: Constrained Bayesian Optimization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  bounds âˆˆ â„â¿Ë£Â² - Adaptive search bounds
        Ï_min - Minimum probability of feasibility
        GP_Y - Gaussian Process for objective
        GP_G - Gaussian Process for constraints
        D - Historical observations {(Î¸áµ¢, Jáµ¢, gáµ¢)}
        
Output: Î¸_next - Next candidate to evaluate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === UPDATE GAUSSIAN PROCESSES ===
2:  X â† [Î¸â‚, Î¸â‚‚, ..., Î¸_|D|]áµ€
3:  Y â† [Jâ‚, Jâ‚‚, ..., J_|D|]áµ€
4:  G â† [gâ‚, gâ‚‚, ..., g_|D|]áµ€
5:  
6:  GP_Y.Fit(X, Y)                          â–· Train objective surrogate
7:  GP_G.Fit(X, G)                          â–· Train constraint surrogate
8:  
9:  // === IDENTIFY BEST FEASIBLE ===
10: D_feas â† {(Î¸, J) âˆˆ D | g â‰¤ 0}
11: if D_feas â‰  âˆ… then
12:     J_best â† min{J | (Î¸, J) âˆˆ D_feas}
13: else
14:     J_best â† min{J | (Î¸, J) âˆˆ D}        â–· Fallback if none feasible
15: end if
16: 
17: // === GENERATE CANDIDATES ===
18: U â† SampleLatinHypercube(bounds, N_candidates)
19: 
20: // === COMPUTE ACQUISITION (EIC) ===
21: for each u âˆˆ U do
22:     
23:     // Expected Improvement
24:     Î¼_Y(u), Ïƒ_Y(u) â† GP_Y.Predict(u)
25:     if Ïƒ_Y(u) > 0 then
26:         Z â† (J_best - Î¼_Y(u) - Î¾) / Ïƒ_Y(u)
27:         EI(u) â† Ïƒ_Y(u) Â· [Z Â· Î¦(Z) + Ï†(Z)]  â–· Î¦: CDF, Ï†: PDF
28:     else
29:         EI(u) â† 0
30:     end if
31:     
32:     // Probability of Feasibility
33:     Î¼_G(u), Ïƒ_G(u) â† GP_G.Predict(u)
34:     if Ïƒ_G(u) > 0 then
35:         PoF(u) â† Î¦(-Î¼_G(u) / Ïƒ_G(u))      â–· P(g(u) â‰¤ 0)
36:     else
37:         PoF(u) â† 1 if Î¼_G(u) â‰¤ 0 else 0
38:     end if
39:     
40:     // Expected Improvement with Constraints
41:     EIC(u) â† EI(u) Â· PoF(u)
42:     
43: end for
44: 
45: // === SELECT BEST ACQUISITION ===
46: Î¸_next â† argmax_u {EIC(u) | PoF(u) â‰¥ Ï_min}
47: 
48: // === FALLBACK IF NO FEASIBLE CANDIDATES ===
49: if âˆ„ u : PoF(u) â‰¥ Ï_min then
50:     Î¸_next â† argmax_u {PoF(u)}           â–· Most likely feasible
51: end if
52: 
53: return Î¸_next


Algorithm 7: BO Preference Nudging
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Î¸_pref - Preferred candidate
        J_pref - Objective value of preferred
        J_other - Objective value of other candidate
        g_pref - Constraint violation of preferred
        strength Î² âˆˆ (0,1) - Nudge strength
        
Output: Updated GP with synthetic observation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === ONLY NUDGE IF PREFERRED IS FEASIBLE ===
2:  if g_pref > 0 then
3:      return                               â–· Skip if infeasible
4:  end if
5:  
6:  // === COMPUTE SYNTHETIC COST ===
7:  Î” â† |J_other - J_pref|                   â–· Performance gap
8:  
9:  if Î” > 0 then
10:     J_pseudo â† J_pref - Î² Â· Î”            â–· Artificially improve
11: else
12:     J_pseudo â† J_pref - Î² Â· 0.01         â–· Small improvement
13: end if
14: 
15: // === ADD SYNTHETIC OBSERVATION TO GP ===
16: GP_Y.AddObservation(Î¸_pref, J_pseudo)    â–· "Better" than reality
17: 
18: // Note: This biases BO to explore near Î¸_pref in future iterations
```

---

## Algorithm 5: Search Space Adaptation

```
Algorithm 8: Refine Search Space (Exploitation)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Î¸_center - Center point for refinement
        shrink_factor Î» âˆˆ (0,1) - Shrinkage ratio
        bounds_current - Current adaptive bounds
        global_bounds - Hard domain limits
        
Output: bounds_new - Refined (smaller) bounds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === COMPUTE NEW RANGE ===
2:  range_current â† bounds_current_max - bounds_current_min
3:  range_new â† Î» Â· range_current
4:  
5:  // === CENTER NEW BOUNDS ===
6:  bounds_new_min â† Î¸_center - range_new / 2
7:  bounds_new_max â† Î¸_center + range_new / 2
8:  
9:  // === CLAMP TO GLOBAL BOUNDS ===
10: bounds_new_min â† MAX(bounds_new_min, global_bounds_min)
11: bounds_new_max â† MIN(bounds_new_max, global_bounds_max)
12: bounds_new_max â† MAX(bounds_new_max, bounds_new_min + Îµ)
13: 
14: // === FOR DE: RESTART POPULATION ===
15: if optimizer = DE then
16:     mutation â† 0.8 Â· mutation            â–· Reduce exploration
17:     population â† InitializePopulation(bounds_new)
18:     population[0] â† Î¸_center             â–· Keep center point
19: end if
20: 
21: return bounds_new


Algorithm 9: Expand Search Space (Exploration)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  expand_factor Î³ > 1 - Expansion ratio
        bounds_current - Current adaptive bounds
        global_bounds - Hard domain limits
        
Output: bounds_new - Expanded (larger) bounds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === COMPUTE CENTER ===
2:  Î¸_center â† (bounds_current_min + bounds_current_max) / 2
3:  
4:  // === COMPUTE NEW RANGE ===
5:  range_current â† bounds_current_max - bounds_current_min
6:  range_new â† Î³ Â· range_current
7:  
8:  // === EXPAND SYMMETRICALLY ===
9:  bounds_new_min â† Î¸_center - range_new / 2
10: bounds_new_max â† Î¸_center + range_new / 2
11: 
12: // === CLAMP TO GLOBAL BOUNDS ===
13: bounds_new_min â† MAX(bounds_new_min, global_bounds_min)
14: bounds_new_max â† MIN(bounds_new_max, global_bounds_max)
15: bounds_new_max â† MAX(bounds_new_max, bounds_new_min + Îµ)
16: 
17: // === FOR DE: RESTART WITH MORE EXPLORATION ===
18: if optimizer = DE then
19:     mutation â† MIN(1.2 Â· mutation, 1.0)  â–· Increase up to max 1.0
20:     population â† InitializePopulation(bounds_new)
21: end if
22: 
23: return bounds_new
```

---

## Algorithm 6: Candidate Injection

```
Algorithm 10: Inject Candidate into DE Population
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Î¸_inject - Candidate to inject
        P - Current DE population
        J, g - Current fitness and violations
        protect_best - Whether to protect best individual
        
Output: Updated population with injected candidate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === CLIP TO VALID BOUNDS ===
2:  Î¸_inject â† CLIP(Î¸_inject, bounds_min, bounds_max)
3:  Î¸_inject â† CLIP(Î¸_inject, global_bounds_min, global_bounds_max)
4:  
5:  // === EVALUATE CANDIDATE ===
6:  J_inject, g_inject â† EvaluatePID(Î¸_inject)
7:  
8:  // === SELECT REPLACEMENT INDEX ===
9:  if protect_best then
10:     idx_best â† argmin_i {J[i] | using feasibility rules}
11:     candidates â† {1,...,N_pop} \ {idx_best}
12: else
13:     candidates â† {1,...,N_pop}
14: end if
15: 
16: idx_replace â† RANDOM_CHOICE(candidates)
17: 
18: // === REPLACE POPULATION MEMBER ===
19: P[idx_replace] â† Î¸_inject
20: J[idx_replace] â† J_inject
21: g[idx_replace] â† g_inject
22: 
23: return P, J, g
```

---

## Algorithm 7: PID Evaluation and Metrics

```
Algorithm 11: PID Controller Evaluation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Î¸ = [K_p, K_i, K_d] - PID parameters
        r(t) - Reference trajectory (target)
        T_sim - Simulation duration
        Î”t - Time step
        u_max - Actuator saturation limit
        
Output: J - Total cost (objective)
        g - Constraint violation
        metrics - Performance metrics
        response - Time-series response
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  // === INITIALIZATION ===
2:  e_integral â† 0
3:  y_prev â† 0
4:  J_total â† 0
5:  u_max_observed â† 0
6:  sat_steps â† 0
7:  response â† âˆ…
8:  
9:  // === SIMULATION LOOP ===
10: for t â† 0 to T_sim step Î”t do
11:     
12:     // === GET CURRENT STATE ===
13:     y(t) â† MeasureSystemOutput()         â–· From physics simulator
14:     
15:     // === COMPUTE ERROR ===
16:     e(t) â† r(t) - y(t)
17:     e_integral â† e_integral + e(t) Â· Î”t
18:     
19:     // === DERIVATIVE-ON-MEASUREMENT ===
20:     Ä—_measured â† (y(t) - y_prev) / Î”t    â–· Reduces derivative kick
21:     
22:     // === PID CONTROL LAW ===
23:     u_raw(t) â† K_p Â· e(t) + K_i Â· e_integral - K_d Â· Ä—_measured
24:     
25:     // === SATURATION HANDLING ===
26:     u_max_observed â† MAX(u_max_observed, |u_raw(t)|)
27:     u(t) â† CLIP(u_raw(t), -u_max, u_max)
28:     
29:     if |u_raw(t)| > u_max then
30:         sat_steps â† sat_steps + 1
31:         
32:         // === ANTI-WINDUP ===
33:         if SIGN(u_raw(t)) = SIGN(e(t)) then
34:             e_integral â† e_integral - e(t) Â· Î”t
35:         end if
36:     end if
37:     
38:     // === APPLY CONTROL ===
39:     ApplyControlInput(u(t))              â–· To physics simulator
40:     StepSimulation(Î”t)
41:     
42:     // === ACCUMULATE COST ===
43:     sat_excess â† MAX(0, |u_raw(t)| - u_max)
44:     J_total â† J_total + e(t)Â² + 0.001Â·u(t)Â² + Î»_satÂ·sat_excessÂ²
45:     
46:     // === RECORD RESPONSE ===
47:     response â† response âˆª {(t, r(t), y(t), u(t))}
48:     y_prev â† y(t)
49:     
50: end for
51: 
52: // === STRICT SATURATION PENALTY ===
53: if u_max_observed > u_max then
54:     excess_ratio â† (u_max_observed - u_max) / u_max
55:     J_total â† J_total + Î»_hard Â· (1 + excess_ratio)
56: end if
57: 
58: // === NORMALIZE COST ===
59: J â† J_total / (T_sim / Î”t)
60: 
61: // === CONSTRAINT VIOLATION ===
62: g â† u_max_observed - u_max               â–· g â‰¤ 0 is feasible
63: 
64: // === PERFORMANCE METRICS ===
65: metrics â† ComputePerformanceMetrics(response)
66: 
67: return J, g, metrics, response


Algorithm 12: Performance Metrics Computation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  response - Time series {(t, r, y, u)}
        r_target - Target value
        
Output: metrics - {overshoot, rise_time, settling_time}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1:  y_max â† MAX{y | (t, r, y, u) âˆˆ response}
2:  
3:  // === OVERSHOOT ===
4:  if y_max > r_target then
5:      overshoot â† 100 Â· (y_max - r_target) / r_target
6:  else
7:      overshoot â† 0
8:  end if
9:  
10: // === RISE TIME (10% to 90%) ===
11: t_10 â† MIN{t | y(t) â‰¥ 0.1 Â· r_target}
12: t_90 â† MIN{t | y(t) â‰¥ 0.9 Â· r_target}
13: rise_time â† t_90 - t_10
14: 
15: // === SETTLING TIME (Â±5% of target) ===
16: tolerance â† 0.05 Â· r_target
17: last_violation â† MAX{t | |y(t) - r_target| > tolerance}
18: settling_time â† last_violation
19: 
20: return {overshoot, rise_time, settling_time}
```

---

## Key Algorithmic Features

### 1. **Dual-Algorithm Synergy**
- DE provides robust global exploration via population-based evolution
- BO leverages probabilistic surrogate models for sample-efficient local optimization
- Bidirectional knowledge sharing: BO learns from all DE evaluations

### 2. **Adaptive Preference Learning**
- Exponential moving average (Î± = 0.3) balances responsiveness and stability
- Anchor injection guides both optimizers toward human-preferred regions
- Synthetic observation nudging biases BO's Gaussian Process toward preferences

### 3. **Feasibility-Aware Selection**
- Deb's constraint handling: feasible solutions always dominate infeasible ones
- Among feasible: minimize objective; among infeasible: minimize violation
- Ensures physically realizable controllers (actuator saturation constraints)

### 4. **Multi-Modal Feedback Mechanism**
- **Preference** (PREFER_DE/BO): Update weights, inject anchors, cross-pollinate
- **Refinement** (TIE): Shrink bounds by 50%, reduce mutation, intensify local search
- **Expansion** (REJECT): Expand bounds by 50%, increase mutation, explore new regions
- **Termination**: Auto-stop when performance targets and constraints are satisfied

### 5. **Hierarchical Bounds Management**
- **Global bounds**: Hard parameter limits, never violated
- **Adaptive bounds**: Dynamically adjusted per human feedback
- Ensures mathematical validity while enabling focused search

---

## Computational Complexity

| Component | Complexity | Notes |
|-----------|------------|-------|
| DE Evolution | ğ’ª(N_pop Â· n) | n = parameter dimensions |
| BO GP Training | ğ’ª(N_obsÂ³) | N_obs = number of observations |
| BO Acquisition | ğ’ª(N_cand Â· n) | N_cand = candidate samples |
| PID Simulation | ğ’ª(T_sim/Î”t) | Physics simulation steps |
| Overall Iteration | ğ’ª(N_pop Â· T_sim/Î”t) | Dominated by simulations |

---

## Notation Summary

| Symbol | Meaning |
|--------|---------|
| Î¸ | PID parameter vector [K_p, K_i, K_d] |
| J | Objective function (cost) |
| g | Constraint violation (g â‰¤ 0 is feasible) |
| w | Preference weight vector |
| Î± | Preference learning rate |
| F | DE mutation factor |
| Ï_min | Minimum probability of feasibility (BO) |
| Î» | Shrink/expand factor for bounds |
| GP | Gaussian Process |
| EI | Expected Improvement |
| PoF | Probability of Feasibility |
| EIC | Expected Improvement with Constraints |

---

## References

**Algorithmic Foundations:**
1. Storn & Price (1997) - Differential Evolution
2. Mockus (1975) - Bayesian Optimization
3. Deb (2000) - Constraint handling in evolutionary algorithms
4. Schonlau et al. (1998) - Expected Improvement acquisition
5. Gardner et al. (2014) - Constrained Bayesian Optimization

**PID Control:**
6. Ã…strÃ¶m & HÃ¤gglund (1995) - PID Controllers: Theory, Design, and Tuning
7. Anti-windup techniques for saturating controllers

---

*This pseudocode provides a complete, unambiguous specification of the Mixed HIL optimization framework suitable for reproduction in academic publications.*
